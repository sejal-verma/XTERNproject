{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470ea9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FileFinder' object has no attribute 'find_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     13\u001b[39m sentiment = pipeline(\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msentiment-analysis\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mdistilbert/distilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# X (Twitter) via snscrape *as a library* (no subprocess)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnscrape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m twitter \u001b[38;5;28;01mas\u001b[39;00m sntwitter\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structured/educational/PUI/25-26/FALL25/MISO-hackathon/XTERNproject/.MISO-venv/lib/python3.13/site-packages/snscrape/modules/__init__.py:17\u001b[39m\n\u001b[32m     13\u001b[39m \t\tmodule = importer.find_module(moduleName).load_module(moduleName)\n\u001b[32m     14\u001b[39m \t\t\u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] = module\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m_import_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/structured/educational/PUI/25-26/FALL25/MISO-hackathon/XTERNproject/.MISO-venv/lib/python3.13/site-packages/snscrape/modules/__init__.py:13\u001b[39m, in \u001b[36m_import_modules\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m moduleNameWithoutPrefix = moduleName[prefixLen:]\n\u001b[32m     12\u001b[39m __all__.append(moduleNameWithoutPrefix)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m module = \u001b[43mimporter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_module\u001b[49m(moduleName).load_module(moduleName)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] = module\n",
      "\u001b[31mAttributeError\u001b[39m: 'FileFinder' object has no attribute 'find_module'"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import os, re, hashlib, time, json, datetime as dt\n",
    "from collections import defaultdict\n",
    "\n",
    "# HTTP / parsing / data\n",
    "import requests, feedparser, pandas as pd\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# X (Twitter) via snscrape *as a library* (no subprocess)\n",
    "from snscrape.modules import twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = [\n",
    "  \"data center\",\"ai campus\",\"hyperscale\",\"substation\",\"megawatt\",\n",
    "  \"moratorium\",\"rezoning\",\"pud\",\"special use\",\"rate rider\",\"economic development rate\"\n",
    "]\n",
    "\n",
    "# Minimal gazetteer: expand as you learn hotspots\n",
    "COUNTY_CITIES = {\n",
    "  \"IN:Johnson County\": {\"cities\": [\"Greenwood\",\"Franklin\",\"Whiteland\"]},\n",
    "  \"MN:Scott County\":   {\"cities\": [\"Shakopee\",\"Prior Lake\"]},\n",
    "  # add more counties/cities in the MISO footprint as you discover signals\n",
    "}\n",
    "\n",
    "def has_kw(text: str) -> bool:\n",
    "    low = (text or \"\").lower()\n",
    "    return any(k in low for k in KEYWORDS)\n",
    "\n",
    "def sha(u: str) -> str:\n",
    "    return hashlib.sha256((u or \"\").encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aeaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdelt_search(query: str, maxrecords: int = 100):\n",
    "    \"\"\"Return list of {title,url,seendate} from GDELT or Google News RSS fallback.\"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"mode\": \"ArtList\",\n",
    "        \"maxrecords\": maxrecords,\n",
    "        \"format\": \"JSON\",\n",
    "        \"sort\": \"DateDesc\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        # Some failures return HTML, not JSON\n",
    "        if \"application/json\" not in (r.headers.get(\"content-type\",\"\")):\n",
    "            raise ValueError(\"Non-JSON from GDELT\")\n",
    "        data = r.json()\n",
    "        arts = data.get(\"articles\", [])\n",
    "        return [\n",
    "            {\"title\": a.get(\"title\",\"\"), \"url\": a.get(\"url\",\"\"), \"seendate\": a.get(\"seendate\",\"\")}\n",
    "            for a in arts\n",
    "            if a.get(\"title\") and a.get(\"url\")\n",
    "        ]\n",
    "    except Exception:\n",
    "        # Fallback to Google News RSS (robust and fast)\n",
    "        rss = feedparser.parse(\n",
    "            f\"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=en-US&gl=US&ceid=US:en\"\n",
    "        )\n",
    "        out = []\n",
    "        for e in rss.entries[:maxrecords]:\n",
    "            out.append({\n",
    "                \"title\": e.title,\n",
    "                \"url\": e.link,\n",
    "                \"seendate\": getattr(e, \"published\", \"\")\n",
    "            })\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_gdelt(items):\n",
    "    rows = []\n",
    "    for a in items:\n",
    "        t = a.get(\"title\",\"\"); u = a.get(\"url\",\"\"); sd = a.get(\"seendate\",\"\")\n",
    "        if not t or not u:\n",
    "            continue\n",
    "        if not has_kw(t):\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"source\": \"news\",\n",
    "            \"title\": t,\n",
    "            \"text\": f\"{sd} {t}\",\n",
    "            \"url\": u,\n",
    "            \"ts\": sd\n",
    "        })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c339cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_x(keyword: str, limit: int = 200):\n",
    "    tweets = []\n",
    "    for i, tw in enumerate(sntwitter.TwitterSearchScraper(keyword).get_items()):\n",
    "        if i >= limit: break\n",
    "        tweets.append({\n",
    "            \"content\": getattr(tw, \"rawContent\", None) or getattr(tw, \"content\", \"\") or \"\",\n",
    "            \"id\": tw.id,\n",
    "            \"date\": tw.date.isoformat() if getattr(tw, \"date\", None) else \"\",\n",
    "        })\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d52973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_x(tweets):\n",
    "    rows = []\n",
    "    for tw in tweets:\n",
    "        t = tw.get(\"content\",\"\")\n",
    "        if not has_kw(t):\n",
    "            continue\n",
    "        u = f\"https://x.com/i/web/status/{tw.get('id')}\"\n",
    "        rows.append({\n",
    "            \"source\": \"x\",\n",
    "            \"title\": (t[:120] + \"…\") if len(t) > 120 else t,\n",
    "            \"text\": t,\n",
    "            \"url\": u,\n",
    "            \"ts\": tw.get(\"date\",\"\")\n",
    "        })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bdbde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich(row):\n",
    "    # NER\n",
    "    doc = nlp(row[\"text\"])\n",
    "    row[\"entities\"] = [e.text for e in doc.ents if e.label_ in (\"ORG\",\"GPE\",\"LOC\")]\n",
    "\n",
    "    # County heuristic\n",
    "    row[\"county\"] = None\n",
    "    lower_text = row[\"text\"].lower()\n",
    "    for county, bag in COUNTY_CITIES.items():\n",
    "        if any(city.lower() in lower_text for city in bag[\"cities\"]):\n",
    "            row[\"county\"] = county\n",
    "            break\n",
    "\n",
    "    # Sentiment (explicit model chosen above)\n",
    "    s = sentiment(row[\"text\"][:512])[0]  # {'label': 'POSITIVE'|'NEGATIVE', 'score':...}\n",
    "    row[\"polarity\"] = (s[\"label\"] or \"\").lower()\n",
    "    row[\"polarity_score\"] = s[\"score\"]\n",
    "\n",
    "    # Stance (cheap rules → upgrade later to a classifier)\n",
    "    if any(w in lower_text for w in [\"moratorium\",\"ban\",\"halt\",\"suspend\"]):\n",
    "        stance = \"oppose\"\n",
    "    elif any(w in lower_text for w in [\"approved\",\"incentive\",\"tax abatement\",\"wins vote\",\"passes\"]):\n",
    "        stance = \"support\"\n",
    "    else:\n",
    "        stance = \"neutral\"\n",
    "    row[\"stance\"] = stance\n",
    "\n",
    "    # Urgency flag\n",
    "    row[\"urgency\"] = any(w in lower_text for w in [\n",
    "        \"agenda\", \"ordinance\", \"hearing\", \"vote\", \"1st reading\", \"first reading\", \"second reading\", \"rezoning\"\n",
    "    ])\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebab582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # 1) Collect\n",
    "    q_news = '(\"data center\" OR hyperscale) sourcecountry:US'\n",
    "    news_items = gdelt_search(q_news, maxrecords=150)\n",
    "    news_rows = normalize_gdelt(news_items)\n",
    "\n",
    "    q_x = '(\"data center\" OR hyperscale) (moratorium OR rezoning OR substation) lang:en'\n",
    "    x_items = scrape_x(q_x, limit=300)\n",
    "    x_rows = normalize_x(x_items)\n",
    "\n",
    "    all_rows = news_rows + x_rows\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(), pd.Series(dtype=float)\n",
    "\n",
    "    # 2) Deduplicate & enrich\n",
    "    df = pd.DataFrame(all_rows).drop_duplicates(subset=\"url\").reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        return df, pd.Series(dtype=float)\n",
    "\n",
    "    df = df.apply(enrich, axis=1, result_type=\"expand\")\n",
    "\n",
    "    # 3) Score each row\n",
    "    def score(r):\n",
    "        w = 0.0\n",
    "        w += 1.0 if r[\"polarity\"]==\"negative\" else 0.3 if r[\"polarity\"]==\"positive\" else 0.1\n",
    "        w += 1.2 if r[\"stance\"]==\"oppose\"  else 0.6 if r[\"stance\"]==\"support\"  else 0.0\n",
    "        w += 1.5 if r[\"urgency\"] else 0.0\n",
    "        w += 0.5 if r[\"county\"] else 0.0\n",
    "        return w\n",
    "\n",
    "    df[\"emi_component\"] = df.apply(score, axis=1)\n",
    "\n",
    "    # 4) Aggregate by county (drop NAs so you see mapped ones)\n",
    "    agg = (df.groupby(\"county\", dropna=True)[\"emi_component\"]\n",
    "             .sum()\n",
    "             .sort_values(ascending=False))\n",
    "\n",
    "    return df, agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be999ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top counties (EMI):\")\n",
    "print(agg.head(5))\n",
    "\n",
    "for county in agg.head(3).index:\n",
    "    print(f\"\\n== {county} ==\")\n",
    "    print(\n",
    "        df[df[\"county\"]==county][[\"source\",\"title\",\"url\",\"stance\",\"urgency\"]]\n",
    "          .head(3)\n",
    "          .to_markdown(index=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619881d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MISO-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
