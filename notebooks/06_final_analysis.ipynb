{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99651c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from Reddit and X.\n",
      "\n",
      "--- Analysis Results ---\n",
      "Top Reddit Keywords: ['centers', 'industry', 'crisis', '2025', 'energy', 'coal', 'help', 'solar', 'electric', '100', 'gas', 'uk', 'power', 'trump', 'electricity', 'prices', 'new', 'utility', 'grid', 'data']\n",
      "Top X Keywords:   ['newsom', 'https', 'centers', 'cost', 'industry', 'rt', 'crisis', 'backup', 'demand', 'low', '2025', 'available', 'families', 'income', 'energy', 'coal', 'help', 'reliability', 'solar', 'electric', 'incentive', 'install', 'future', 'october', '100', 'mayorofla', 'gas', 'uk', 'capacity', 'power', 'self', 'trump', 'electricity', 'city', 'generation', 'prices', 'allowing', 'austin', 'new', 'texas', 'utility', 'lower', 'control', 'cover', 'program', 'grid', 'amp', 'need', 'bills', 'data']\n",
      "\n",
      " Shared Signal Keywords: ['centers', 'industry', 'crisis', '2025', 'energy', 'coal', 'help', 'solar', 'electric', '100', 'gas', 'uk', 'power', 'trump', 'electricity', 'prices', 'new', 'utility', 'grid', 'data']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "\n",
    "# --- 1. LOAD ALL YOUR DATA ---\n",
    "reddit_path = '../data/raw/reddit_titles_filtered.csv'\n",
    "twitter_path = '../data/raw/twitter_data.csv'\n",
    "\n",
    "# Load the two datasets you created\n",
    "df_reddit = pd.read_csv(reddit_path)\n",
    "df_twitter = pd.read_csv(twitter_path)\n",
    "\n",
    "print(\"Successfully loaded data from Reddit and X.\")\n",
    "\n",
    "# --- 2. FIND SHARED KEYWORDS (THE SIGNAL) ---\n",
    "reddit_titles = df_reddit['title'].dropna()\n",
    "twitter_titles = df_twitter['title'].dropna()\n",
    "    \n",
    "try:\n",
    "    # We use a single vectorizer to learn the vocabulary from ALL titles\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=50)\n",
    "    vectorizer.fit(pd.concat([reddit_titles, twitter_titles]))\n",
    "\n",
    "    # Find the keywords present in each source\n",
    "    reddit_keywords = {word for word in vectorizer.get_feature_names_out() if vectorizer.transform(reddit_titles).toarray().sum(axis=0)[vectorizer.vocabulary_[word]] > 0}\n",
    "    twitter_keywords = {word for word in vectorizer.get_feature_names_out() if vectorizer.transform(twitter_titles).toarray().sum(axis=0)[vectorizer.vocabulary_[word]] > 0}\n",
    "    \n",
    "    # Find the keywords that appear in BOTH lists\n",
    "    shared_keywords = reddit_keywords.intersection(twitter_keywords)\n",
    "\n",
    "    print(\"\\n--- Analysis Results ---\")\n",
    "    print(f\"Top Reddit Keywords: {list(reddit_keywords)}\")\n",
    "    print(f\"Top X Keywords:   {list(twitter_keywords)}\")\n",
    "    print(\"\\n Shared Signal Keywords:\", list(shared_keywords) if shared_keywords else \"None found.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\\n--- Analysis Error --- \\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6acdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
